{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1a8Idt2iYgc"
      },
      "outputs": [],
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "# word_tokenize is based on TreeBankWordTokenizer which uses Regular Expressions (https://www.nltk.org/_modules/nltk/tokenize.html#word_tokenize , https://www.nltk.org/_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer)\n",
        "# PunktSentenceTokenizer uses an unsupervised algorithm (https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepares preprocessed sequences to be POS-tagged with anchor and cleaned afterwards, actual POS-tagging occurs in WSL (linux)\n"
      ],
      "metadata": {
        "id": "7V9jie83140s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY3rIG1mjy1H"
      },
      "outputs": [],
      "source": [
        "with open('combined.dev') as infile:\n",
        "  combined = infile.read()\n",
        "\n",
        "comb_sequences = combined.split(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0qSo2MHjxdc"
      },
      "outputs": [],
      "source": [
        "# train tokenizer on dev set\n",
        "sent_tokenizer = PunktSentenceTokenizer(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRhhpteQj4gc"
      },
      "outputs": [],
      "source": [
        "# tokenize dev set\n",
        "\n",
        "full_string = ''\n",
        "\n",
        "for seq in comb_sequences:\n",
        "  sentences = sent_tokenizer.tokenize(seq)\n",
        "  for sentence in sentences:\n",
        "    tagged = pos_tag(word_tokenize(sentence), tagset='universal')\n",
        "    tokens = ''\n",
        "    posses = ''\n",
        "    for (token, pos) in tagged:\n",
        "      tokens += token + ' '\n",
        "      posses += pos + ' '\n",
        "    full_string += tokens + \"\\t\" + posses + \"\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2uS2Mk1mjf5"
      },
      "outputs": [],
      "source": [
        "with open('tagged_combined.dev', 'w') as outfile:\n",
        "  outfile.write(full_string)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('combined.train') as infile:\n",
        "  combined = infile.read()\n",
        "\n",
        "comb_sequences = combined.split(\"\\n\")"
      ],
      "metadata": {
        "id": "-copA_X3gmvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJenHHZtmnAs"
      },
      "outputs": [],
      "source": [
        "# train tokenizer on train set\n",
        "sent_tokenizer = PunktSentenceTokenizer(combined)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize preprocessed sequences\n",
        "\n",
        "full_string = ''\n",
        "\n",
        "for seq in comb_sequences:\n",
        "  sentences = sent_tokenizer.tokenize(seq)\n",
        "  for sentence in sentences:\n",
        "    tokenized = word_tokenize(sentence)\n",
        "    tokens = \" \".join(tokenized)\n",
        "    full_string += tokens + \"\\n\"\n",
        "\n",
        "with open('tokenized_combined.train', 'w') as outfile:\n",
        "  outfile.write(full_string)"
      ],
      "metadata": {
        "id": "_evocFfkhRR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare tokenized sequences to be POS-tagged\n",
        "\n",
        "with open('tokenized_combined.train') as infile:\n",
        "  combined = infile.read().split(\"\\n\")\n",
        "\n",
        "full_text = ''\n",
        "for seq in combined:\n",
        "  tokens = seq.split(\" \")\n",
        "  for token in tokens:\n",
        "    full_text += (token + \"__<label>__N \")\n",
        "\n",
        "  full_text += \"\\n\"\n",
        "\n",
        "with open('tagged_combined.train', 'w') as outfile:\n",
        "  outfile.write(full_text)"
      ],
      "metadata": {
        "id": "oM2CWjMboTvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manually assigned POS tag per anchor word\n",
        "\n",
        "tag_to_pos = { \"let\" : \"VERB\",\n",
        " \"MCH\" :\"X\",\n",
        " \"are\" : \"VERB\",\n",
        " \"any\" : \"DET\",\n",
        " \"we\" : \"PRON\",\n",
        " \"?\" :\".\",\n",
        " \"in\" : \"ADP\",\n",
        " \"anything\" : \"NOUN\",\n",
        " \"used\" : \"VERB\",\n",
        " \"*\" : \".\",\n",
        " \"house\" : \"NOUN\",\n",
        " \"A\" : \"DET\",\n",
        "}"
      ],
      "metadata": {
        "id": "wYAhutSIqNgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup(filepath):\n",
        "  '''\n",
        "  Take POS-tagged sequences, replace anchor words with POS tags, save to file\n",
        "  '''\n",
        "\n",
        "  with open(filepath) as infile:\n",
        "    sequences = infile.read().split(\"\\n\\n\")\n",
        "\n",
        "  cleaned_sequences = []\n",
        "  for seq in sequences:\n",
        "    words = []\n",
        "    pos = []\n",
        "    tokens = seq.split(\"\\n\")\n",
        "    for token in tokens:\n",
        "      tags = token.split(\" \")\n",
        "      if len(tags) == 4:\n",
        "        words.append(tags[0])\n",
        "        pos.append(tag_to_pos[tags[3]])\n",
        "    clean_string = \" \".join(words) + \"\\t\" + \" \".join(pos)\n",
        "    cleaned_sequences.append(clean_string)\n",
        "\n",
        "  new_filepath = 'clean_' + filepath\n",
        "  with open(new_filepath, 'w') as outfile:\n",
        "    outfile.write(\"\\n\".join(cleaned_sequences))"
      ],
      "metadata": {
        "id": "hscJE70jpmF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleanup(\"pred3\")"
      ],
      "metadata": {
        "id": "lCfltsSP3eu7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}