{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a894iD9jZvc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import PunktSentenceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FCuODFwi658"
      },
      "outputs": [],
      "source": [
        "def check_divisible(numb):\n",
        "  '''\n",
        "  check divisibility of given number, if none return 5\n",
        "  '''\n",
        "  if numb % 4 == 0:\n",
        "    return 4\n",
        "  if numb % 5 == 0:\n",
        "    return 5\n",
        "  if numb % 6 == 0:\n",
        "    return 6\n",
        "  if numb % 7 == 0:\n",
        "    return 7\n",
        "  if numb % 8 == 0:\n",
        "    return 8\n",
        "  if numb % 3 == 0:\n",
        "    return 3\n",
        "  if numb % 2 == 0:\n",
        "    return 2\n",
        "  if numb % 9 == 0:\n",
        "    return 9\n",
        "  if numb % 10 == 0:\n",
        "    return 10\n",
        "  return 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunker(seq, size):\n",
        "  '''\n",
        "  split seq into chunks of equal size (and rest)\n",
        "  '''\n",
        "  return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJxSerH3i5au"
      },
      "outputs": [],
      "source": [
        "def paragraphs_to_sequences(subset, sent_tokenizer):\n",
        "  '''\n",
        "  Splits paragraphs into sequences\n",
        "  If less than 6 sentences: one sequence\n",
        "  if between 6 and 20 sentences: two sequences\n",
        "  If more: Preferably equal size sequences (using check_divisible)\n",
        "\n",
        "  Returns a list of sequences\n",
        "  '''\n",
        "  sequences = []\n",
        "\n",
        "\n",
        "  for text in subset:\n",
        "    if len(text) <= 2:\n",
        "    #if len(text.split(\" \")) < 1:\n",
        "      continue\n",
        "\n",
        "    elif len(text.split(\"\\n\")) <= 10:\n",
        "      sequences.append(text)\n",
        "\n",
        "\n",
        "    elif len(text.split(\"\\n\")) > 10:\n",
        "      tokenized = sent_tokenizer.tokenize(text)\n",
        "\n",
        "      if len(tokenized) <= 6:\n",
        "        sequences.append(\" \".join(tokenized))\n",
        "\n",
        "      elif 6 < len(tokenized) <= 20:\n",
        "        half = int(len(tokenized)/2)\n",
        "        seq1 = tokenized[:half]\n",
        "        seq2 = tokenized[half:]\n",
        "        sequences.append(\" \".join(seq1))\n",
        "        sequences.append(\" \".join(seq2))\n",
        "\n",
        "\n",
        "      elif len(tokenized) > 20:\n",
        "        numb = check_divisible(len(tokenized))\n",
        "        for seq in chunker(tokenized, numb):\n",
        "          sequences.append(\" \".join(seq))\n",
        "\n",
        "  final_sequences = []\n",
        "  for seq in sequences:\n",
        "    seqnew = seq.replace(\"\\n\", \" \")\n",
        "    final_sequences.append(seqnew)\n",
        "\n",
        "  return final_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZaztew4iJES"
      },
      "outputs": [],
      "source": [
        "def end_punct(dataset):\n",
        "  '''\n",
        "  replace space+punctuation with punctuation\n",
        "  delete '- ' at start of line\n",
        "  '''\n",
        "\n",
        "  prep = []\n",
        "  for line in dataset:\n",
        "    if len(line) > 3:\n",
        "      if line.startswith(\"- \"):\n",
        "        line = line.replace(\"- \", \"\")\n",
        "      if len(line) > 3:\n",
        "        line = re.sub(\" [\\.\\?\\!]$\", line[-1], line)\n",
        "      line = line.strip()\n",
        "      if len(line) > 3:\n",
        "        if line[-1] not in ['!', '?' ,'.' , ',', ':' ,' ;', '\\'', '\\\"', \"-\", \")\" ]:\n",
        "          line += \".\"\n",
        "    prep.append(line)\n",
        "\n",
        "  return prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifvzC6j-lntZ"
      },
      "outputs": [],
      "source": [
        "def wiki_prep(dataset):\n",
        "  '''\n",
        "  Simple Wiki Preprocessing: takes list of lines and returns preprocessed list of lines\n",
        "  remove tabs and '===' markers\n",
        "  '''\n",
        "  prep_wikipedia = []\n",
        "  for line in dataset:\n",
        "    if line.startswith(\"=\"):\n",
        "      continue\n",
        "    elif line.startswith(\" =\"):\n",
        "      continue\n",
        "    else:\n",
        "      line = line.replace(\"\\t\", \"\")\n",
        "      prep_wikipedia.append(line)\n",
        "\n",
        "  return prep_wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN3JkrApqUM6"
      },
      "outputs": [],
      "source": [
        "def switch_prep(dataset):\n",
        "  '''\n",
        "  Switchboard Preprocessing: takes list of lines and returns preprocessed list of lines\n",
        "  remove speaker (A/B) markers\n",
        "  '''\n",
        "  prep = []\n",
        "  for line in dataset:\n",
        "    if line.startswith(\"B:\t\"):\n",
        "      line = line.replace(\"B:\t\", \"\")\n",
        "    elif line.startswith(\"A:\t\"):\n",
        "      line = line.replace(\"A:\t\",\"\")\n",
        "    prep.append(line)\n",
        "\n",
        "  return(prep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpX3dvkTiqf2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split_sents(dataset):\n",
        "  '''\n",
        "  Join 5 sentences/lines together to form a sequence\n",
        "  Takes a dataset and returns list of sequences\n",
        "  '''\n",
        "  sequences = []\n",
        "  size = 5\n",
        "  lines = dataset.split(\"\\n\")\n",
        "  for seq in chunker(lines, size):\n",
        "    sequences.append(\" \".join(seq))\n",
        "\n",
        "  return sequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj0YmcR9tRDv"
      },
      "outputs": [],
      "source": [
        "def prep_childes(dataset):\n",
        "  '''\n",
        "  Childes Preprocessing: takes list of lines and returns preprocessed list of lines\n",
        "  removes speaker markers\n",
        "  capitalizes if not capitalized\n",
        "  '''\n",
        "  prep = []\n",
        "  for line in dataset:\n",
        "    line = line.replace(\"*CHI:\t\", \"\")\n",
        "    line = line.replace(\"*MOT:\t\", \"\")\n",
        "    line = line.replace(\"*COL:\t\", \"\")\n",
        "    if len(line) >= 3:\n",
        "      if line[0].isalpha() and line[0].islower():\n",
        "        line = line.capitalize()\n",
        "    prep.append(line)\n",
        "\n",
        "  prepped = end_punct(prep)\n",
        "  prepped2 = wiki_prep(prepped)\n",
        "\n",
        "  return prepped2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N22_hjx-7R-x"
      },
      "outputs": [],
      "source": [
        "def prep_gutenberg(dataset):\n",
        "  '''\n",
        "  Gutenberg Preprocessing: takes list of lines and returns preprocessed list of lines\n",
        "  removes chapter markers and formatting punctuation\n",
        "  '''\n",
        "  prep = []\n",
        "  for line in dataset:\n",
        "    if line.startswith(\"*      *\"):\n",
        "      line = \"\\n\"\n",
        "    if line.startswith(\"CHAPTER\"):\n",
        "      line = \"\\n\"\n",
        "    if line.startswith(\"*CHAPTER\"):\n",
        "      line = \"\\n\"\n",
        "    line.replace(\"=\", \" \")\n",
        "    line.replace(\"_\", \" \")\n",
        "    prep.append(line)\n",
        "  return prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMeJe63zegPj"
      },
      "source": [
        "### Training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1li1UjmwiIUg"
      },
      "outputs": [],
      "source": [
        "# BNC_SPOKEN preprocessing\n",
        "\n",
        "with open('data/train/bnc_spoken.train') as infile:\n",
        "  bnc = infile.read().split(\"\\n\")\n",
        "\n",
        "prep_bnc = end_punct(bnc)\n",
        "\n",
        "with open('data/train/prep_bnc_spoken.train', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(prep_bnc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPeuEC3iizCL"
      },
      "outputs": [],
      "source": [
        "# BNC_SPOKEN splitting\n",
        "\n",
        "with open('data/train/prep_bnc_spoken.train') as infile:\n",
        "  bnc = infile.read()\n",
        "\n",
        "sequences = split_sents(bnc)\n",
        "\n",
        "with open('data/train/seq_bnc_spoken.train', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHHOUoLClK1-"
      },
      "outputs": [],
      "source": [
        "# OPEN_SUBTITLES preprocessing\n",
        "\n",
        "with open('data/train/open_subtitles.train') as infile:\n",
        "  opensubs = infile.read().split(\"\\n\")\n",
        "\n",
        "prep_subs = end_punct(opensubs)\n",
        "\n",
        "with open('data/train/prep_open_subtitles.train', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(prep_subs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lYhCqP7lQz2"
      },
      "outputs": [],
      "source": [
        "# OPEN_SUBTITLES splitting\n",
        "\n",
        "with open('data/train/prep_open_subtitles.train') as infile:\n",
        "  subs = infile.read()\n",
        "\n",
        "sequences = split_sents(subs)\n",
        "\n",
        "with open('data/train/seq_open_subtitles.train', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoGMItD3l1OJ"
      },
      "outputs": [],
      "source": [
        "# SIMPLE_WIKI preprocessing\n",
        "\n",
        "with open('data/train/simple_wiki.train') as infile:\n",
        "  wiki = infile.read().split(\"\\n\")\n",
        "\n",
        "prep_wiki = wiki_prep(wiki)\n",
        "\n",
        "with open('data/train/prep_simple_wiki.train', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(prep_wiki))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e5w2OgFl1_R"
      },
      "outputs": [],
      "source": [
        "# SIMPLE_WIKI splitting\n",
        "\n",
        "with open('data/train/prep_simple_wiki.train') as infile:\n",
        "  full_wikipedia = infile.read()\n",
        "  wikipedia = full_wikipedia.split(\"\\n\\n\")\n",
        "\n",
        "wiki_tokenizer = PunktSentenceTokenizer(full_wikipedia)\n",
        "\n",
        "sequences = paragraphs_to_sequences(wikipedia, wiki_tokenizer)\n",
        "\n",
        "with open('data/train/seq_simple_wiki.train', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLavcjFZqtml"
      },
      "outputs": [],
      "source": [
        "# SWITCHBOARD preprocessing\n",
        "\n",
        "with open('data/train/switchboard.train') as infile:\n",
        "  switch = infile.read().split(\"\\n\")\n",
        "\n",
        "prep_switch = switch_prep(switch)\n",
        "\n",
        "with open('data/train/prep_switchboard.train', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(prep_switch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEbSxZqGrOB8"
      },
      "outputs": [],
      "source": [
        "# SWITCHBOARD splitting\n",
        "\n",
        "with open('data/train/prep_switchboard.train') as infile:\n",
        "  switch = infile.read()\n",
        "\n",
        "sequences = split_sents(switch)\n",
        "\n",
        "with open('data/train/seq_switchboard.train', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMmz1i84uNp0"
      },
      "outputs": [],
      "source": [
        "# CHILDES preprocessing\n",
        "\n",
        "with open('data/train/childes.train') as infile:\n",
        "  childes = infile.read().split(\"\\n\")\n",
        "\n",
        "prepped = prep_childes(childes)\n",
        "\n",
        "with open('data/train/prep_childes.train', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(prepped))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoNVGSUfurFG"
      },
      "outputs": [],
      "source": [
        "# CHILDES splitting\n",
        "\n",
        "with open('data/train/prep_childes.train') as infile:\n",
        "  childes = infile.read().split(\"\\n\\n\")\n",
        "\n",
        "sequences = []\n",
        "for convo in childes:\n",
        "  seqs = split_sents(convo)\n",
        "  sequences.extend(seqs)\n",
        "\n",
        "with open('data/train/seq_childes.train', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti1dtdmE8BLW"
      },
      "outputs": [],
      "source": [
        "# GUTENBERG preprocessing\n",
        "\n",
        "with open('data/train/gutenberg.train') as infile:\n",
        "  gutenberg = infile.read().split(\"\\n\")\n",
        "\n",
        "prep = prep_gutenberg(gutenberg)\n",
        "\n",
        "with open('data/train/prep_gutenberg.train', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(prep))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb1W-tif8zru"
      },
      "outputs": [],
      "source": [
        "# GUTENBERG splitting\n",
        "\n",
        "with open('data/train/prep_gutenberg.train') as infile:\n",
        "  full_gutenberg = infile.read()\n",
        "\n",
        "gut = full_gutenberg.split(\"\\n\\n\\n\")\n",
        "gutenberg = []\n",
        "for x in gut:\n",
        "  gutenberg.extend(x.split(\"\\n\\n\"))\n",
        "\n",
        "\n",
        "gut_tokenizer = PunktSentenceTokenizer(full_gutenberg)\n",
        "\n",
        "sequences = paragraphs_to_sequences(gutenberg, gut_tokenizer)\n",
        "\n",
        "with open('data/train/seq_gutenberg.train', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OqG0TfYiZgI"
      },
      "outputs": [],
      "source": [
        "# combine all preprocessed+split datasets into one file\n",
        "\n",
        "combined = []\n",
        "\n",
        "all_texts = ['data/train/seq_switchboard.train', 'data/train/seq_bnc_spoken.train', 'data/train/seq_childes.train', 'data/train/seq_gutenberg.train', 'data/train/seq_open_subtitles.train', 'data/train/seq_simple_wiki.train']\n",
        "\n",
        "for text in all_texts:\n",
        "  with open(text) as infile:\n",
        "    sequences = infile.read().split(\"\\n\")\n",
        "    combined.extend(sequences)\n",
        "\n",
        "with open('data/train/combined.train', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(combined))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTLXtGAKeo2y"
      },
      "source": [
        "### Dev Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2Fb0NG5eoB_"
      },
      "outputs": [],
      "source": [
        "# BNC_SPOKEN preprocessing\n",
        "\n",
        "with open('data/dev/bnc_spoken.dev') as infile:\n",
        "  bnc = infile.read().split(\"\\n\")\n",
        "\n",
        "prep_bnc = end_punct(bnc)\n",
        "\n",
        "with open('data/dev/prep_bnc_spoken.dev', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(prep_bnc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoJ-Xg17fIoC"
      },
      "outputs": [],
      "source": [
        "# BNC_SPOKEN splitting\n",
        "\n",
        "with open('data/dev/prep_bnc_spoken.dev') as infile:\n",
        "  bnc = infile.read()\n",
        "\n",
        "sequences = split_sents(bnc)\n",
        "\n",
        "with open('data/dev/seq_bnc_spoken.dev', \"w\") as outfile:\n",
        "  outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtNijzE8fUV3"
      },
      "outputs": [],
      "source": [
        "# OPEN_SUBTITLES preprocessing\n",
        "\n",
        "with open('data/dev/open_subtitles.dev') as infile:\n",
        "  opensubs = infile.read().split(\"\\n\")\n",
        "\n",
        "prep_subs = end_punct(opensubs)\n",
        "\n",
        "with open('data/dev/prep_open_subtitles.dev', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(prep_subs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxZHorogfkSq"
      },
      "outputs": [],
      "source": [
        "# OPEN_SUBTITLES splitting\n",
        "\n",
        "with open('data/dev/prep_open_subtitles.dev') as infile:\n",
        "  subs = infile.read()\n",
        "\n",
        "sequences = split_sents(subs)\n",
        "\n",
        "with open('data/dev/seq_open_subtitles.dev', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHTVW_xvg3xJ"
      },
      "outputs": [],
      "source": [
        "# SIMPLE_WIKI preprocessing\n",
        "\n",
        "with open('data/dev/simple_wiki.dev') as infile:\n",
        "  wiki = infile.read().split(\"\\n\")\n",
        "\n",
        "prep_wiki = wiki_prep(wiki)\n",
        "\n",
        "with open('data/dev/prep_simple_wiki.dev', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(prep_wiki))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Wz1mDZZg_-n"
      },
      "outputs": [],
      "source": [
        "# SIMPLE_WIKI splitting\n",
        "\n",
        "with open('data/dev/prep_simple_wiki.dev') as infile:\n",
        "  full_wikipedia = infile.read()\n",
        "  wikipedia = full_wikipedia.split(\"\\n\\n\")\n",
        "\n",
        "wiki_tokenizer = PunktSentenceTokenizer(full_wikipedia)\n",
        "\n",
        "sequences = paragraphs_to_sequences(wikipedia, wiki_tokenizer)\n",
        "\n",
        "with open('data/dev/seq_simple_wiki.dev', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfXjQ4auhFLD"
      },
      "outputs": [],
      "source": [
        "# SWITCHBOARD preprocessing\n",
        "\n",
        "with open('data/dev/switchboard.dev') as infile:\n",
        "  switch = infile.read().split(\"\\n\")\n",
        "\n",
        "prep_switch = switch_prep(switch)\n",
        "\n",
        "with open('data/dev/prep_switchboard.dev', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(prep_switch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDbnF-5ahKyJ"
      },
      "outputs": [],
      "source": [
        "# SWITCHBOARD splitting\n",
        "\n",
        "with open('data/dev/prep_switchboard.dev') as infile:\n",
        "  switch = infile.read()\n",
        "\n",
        "sequences = split_sents(switch)\n",
        "\n",
        "with open('data/dev/seq_switchboard.dev', \"w\") as outfile:\n",
        "   outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR8OP0QyhRkI"
      },
      "outputs": [],
      "source": [
        "# CHILDES preprocessing\n",
        "\n",
        "with open('data/dev/childes.dev') as infile:\n",
        "  childes = infile.read().split(\"\\n\")\n",
        "\n",
        "prepped = prep_childes(childes)\n",
        "\n",
        "with open('data/dev/prep_childes.dev', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(prepped))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7JPML04hYhp"
      },
      "outputs": [],
      "source": [
        "# CHILDES splitting\n",
        "\n",
        "with open('data/dev/prep_childes.dev') as infile:\n",
        "  childes = infile.read().split(\"\\n\\n\")\n",
        "\n",
        "sequences = []\n",
        "for convo in childes:\n",
        "  seqs = split_sents(convo)\n",
        "  sequences.extend(seqs)\n",
        "\n",
        "with open('data/dev/seq_childes.dev', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyDEEiUehci7"
      },
      "outputs": [],
      "source": [
        "# GUTENBERG preprocessing\n",
        "\n",
        "with open('data/dev/gutenberg.dev') as infile:\n",
        "  gutenberg = infile.read().split(\"\\n\")\n",
        "\n",
        "prep = prep_gutenberg(gutenberg)\n",
        "\n",
        "with open('data/dev/prep_gutenberg.dev', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(prep))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWhRIoVEhhCk"
      },
      "outputs": [],
      "source": [
        "# GUTENBERG splitting\n",
        "\n",
        "with open('data/dev/prep_gutenberg.dev') as infile:\n",
        "  full_gutenberg = infile.read()\n",
        "\n",
        "gut = full_gutenberg.split(\"\\n\\n\\n\")\n",
        "gutenberg = []\n",
        "for x in gut:\n",
        "  gutenberg.extend(x.split(\"\\n\\n\"))\n",
        "\n",
        "\n",
        "gut_tokenizer = PunktSentenceTokenizer(full_gutenberg)\n",
        "\n",
        "sequences = paragraphs_to_sequences(gutenberg, gut_tokenizer)\n",
        "\n",
        "with open('data/dev/seq_gutenberg.dev', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAZwRR1ShxiB"
      },
      "outputs": [],
      "source": [
        "# combine all preprocessed+split datasets into one file\n",
        "\n",
        "combined = []\n",
        "\n",
        "all_texts = ['data/dev/seq_switchboard.dev', 'data/dev/seq_bnc_spoken.dev', 'data/dev/seq_childes.dev', 'data/dev/seq_gutenberg.dev', 'data/dev/seq_open_subtitles.dev', 'data/dev/seq_simple_wiki.dev']\n",
        "\n",
        "for text in all_texts:\n",
        "  with open(text) as infile:\n",
        "    sequences = infile.read().split(\"\\n\")\n",
        "    combined.extend(sequences)\n",
        "\n",
        "with open('data/dev/combined.dev', 'w') as outfile:\n",
        "  outfile.write(\"\\n\".join(combined))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
