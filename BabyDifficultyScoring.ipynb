{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd0LeUTUDY0u"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score1(text, pos, child=False):\n",
        "  ''' Identifies pronoun case functor '''\n",
        "  pronouns = ['she', 'her', 'herself', 'he', 'him', 'himself', 'they', 'them', 'themselves', 'we', 'us', 'ourselves', 'ourself' ]\n",
        "  returnal = 0\n",
        "  words = text.split(\" \")\n",
        "  pos_list = pos.split(\" \")\n",
        "  if \"PRON\" in pos_list:\n",
        "    indices = [i for i, x in enumerate(pos_list) if x == \"PRON\"]\n",
        "    for inx in indices:\n",
        "      if words[inx] in pronouns:\n",
        "        if child:\n",
        "          returnal += 0\n",
        "        else: returnal += 1\n",
        "\n",
        "  return returnal\n"
      ],
      "metadata": {
        "id": "I4HsY3PVDww9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score2(text, pos, child=False):\n",
        "  ''' Identifies article functor '''\n",
        "  returnal = 0\n",
        "  articles = [\"a\", \"the\", \"an\"]\n",
        "  for art in articles:\n",
        "    if art in text.split(\" \"):\n",
        "      if child:\n",
        "        returnal += 5\n",
        "      else: returnal += 2\n",
        "  return returnal\n",
        "  #Article"
      ],
      "metadata": {
        "id": "K9LY9XCeD6kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score3(text, pos, child=False):\n",
        "  ''' Identifies contractible copula functor '''\n",
        "  returnal = 0\n",
        "  words = text.split(\" \")\n",
        "  if \"'s\" in words:\n",
        "    indices = [i for i, x in enumerate(words) if x == \"'s\"]\n",
        "    for inx in indices:\n",
        "      pos_list = pos.split(\" \")\n",
        "      if pos_list[inx] == \"VERB\":\n",
        "        if child:\n",
        "          returnal += 6\n",
        "        else: returnal += 3\n",
        "\n",
        "  return returnal\n",
        "  #contractible copula"
      ],
      "metadata": {
        "id": "MBVu8_lIEG3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score4(text, pos, child=False):\n",
        "  ''' Identifies progressive functor '''\n",
        "  returnal = 0\n",
        "  if re.search('VERB', pos):\n",
        "    pos_list = pos.split(\" \")\n",
        "    indices = [i for i, x in enumerate(pos_list) if x == 'VERB']\n",
        "    for inx in indices:\n",
        "      inx_verb = text.split(\" \")[inx]\n",
        "      if inx_verb.endswith('ing'):\n",
        "        if child:\n",
        "          returnal += 2\n",
        "        else: returnal += 4\n",
        "\n",
        "  return returnal\n",
        "  # -ing\n",
        "  #V+ing"
      ],
      "metadata": {
        "id": "RUpIPFZoD_do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score5(text, pos, child=False):\n",
        "  ''' Identifies plural functor '''\n",
        "  returnal = 0\n",
        "  avoid = ['s', 'e', 'u'] # exclude -ss (pass, mess) -es (long plural) -us (octopus, bus)\n",
        "  if re.search('NOUN', pos):\n",
        "    pos_list = pos.split(\" \")\n",
        "    indices = [i for i, x in enumerate(pos_list) if x == 'NOUN']\n",
        "    for inx in indices:\n",
        "      inx_noun = text.split(\" \")[inx]\n",
        "      if len(inx_noun) > 1:\n",
        "        if inx_noun.endswith('s') and inx_noun[-2] not in avoid:\n",
        "          if child:\n",
        "            returnal += 1\n",
        "          else: returnal += 5\n",
        "\n",
        "  return returnal\n",
        "  #plural\n",
        "  #NP+pl"
      ],
      "metadata": {
        "id": "KE9kFZQ9EElC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score6(text, pos, child=False):\n",
        "  ''' Identifies contractible auxiliary functor '''\n",
        "  if re.search('VERB VERB', pos):\n",
        "    auxes = re.findall(\"(('s)|('re)) \\w*ing\", text)\n",
        "    if child:\n",
        "      return (9*len(auxes))\n",
        "    return (6*len(auxes))\n",
        "\n",
        "  return 0\n",
        "  #contractible auxiliary\n",
        "  # -be-V+ing\n",
        "  # only triggered once"
      ],
      "metadata": {
        "id": "U2HCcf8YENNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score7(text, pos, child=False):\n",
        "  ''' Identifies past regular functor '''\n",
        "  returnal = 0\n",
        "  if re.search('VERB', pos):\n",
        "    pos_list = pos.split(\" \")\n",
        "    indices = [i for i, x in enumerate(pos_list) if x == 'VERB']\n",
        "    for inx in indices:\n",
        "      inx_verb = text.split(\" \")[inx]\n",
        "      if inx_verb.endswith('ed'):\n",
        "        if child:\n",
        "          returnal += 3\n",
        "        else: returnal += 7\n",
        "\n",
        "  return returnal\n",
        "  #past regular\n",
        "  #NP/Pron - (have) - V+pst - NP/Pron"
      ],
      "metadata": {
        "id": "8MpPSrcZEgXI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('irregular_verbs.txt') as infile:\n",
        "  ''' Identifies past irregular functor '''\n",
        "  irregular_verbs = infile.read().split(\"\\n\")\n",
        "\n",
        "def score8(text, pos, child=False):\n",
        "  returnal = 0\n",
        "\n",
        "  if re.search('VERB', pos):\n",
        "    pos_list = pos.split(\" \")\n",
        "    indices = [i for i, x in enumerate(pos_list) if x == 'VERB']\n",
        "    for inx in indices:\n",
        "      inx_verb = text.split(\" \")[inx]\n",
        "      if inx_verb in irregular_verbs:\n",
        "        if child:\n",
        "          returnal += 4\n",
        "        else: returnal += 8\n",
        "\n",
        "  return returnal\n",
        "  #past irregular\n",
        "  #NP/pron - V+pst - NP/Pron"
      ],
      "metadata": {
        "id": "7ElJXSroEqyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score9(text, pos, child=False):\n",
        "  ''' Identifies long plural functor '''\n",
        "  returnal = 0\n",
        "  if re.search('NOUN', pos):\n",
        "    pos_list = pos.split(\" \")\n",
        "    indices = [i for i, x in enumerate(pos_list) if x == 'NOUN']\n",
        "    for inx in indices:\n",
        "      inx_noun = text.split(\" \")[inx]\n",
        "      if inx_noun.endswith('es'):\n",
        "        if child:\n",
        "          returnal += 1\n",
        "        else: returnal += 9\n",
        "\n",
        "  return returnal\n",
        "  #long plural\n",
        "  #houses\n"
      ],
      "metadata": {
        "id": "c9iZx8DAExpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score10(text, pos, child=False):\n",
        "  ''' Identifies possessive functor '''\n",
        "  returnal = 0\n",
        "  if re.search('NOUN', pos):\n",
        "    pos_list = pos.split(\" \")\n",
        "    words = text.split(\" \")\n",
        "    indices = [i for i, x in enumerate(pos_list) if x == 'NOUN']\n",
        "    for inx in indices:\n",
        "      if inx != len(words) -1:\n",
        "        if words[inx + 1] == \"'s\":\n",
        "          if pos_list[inx + 1] != 'VERB':\n",
        "            if child:\n",
        "              returnal += 7\n",
        "            else: returnal += 10\n",
        "\n",
        "  return returnal\n",
        "\n",
        "  #possessive 's\n",
        "  #Det- (Adj) - N + poss-(N)"
      ],
      "metadata": {
        "id": "g-ktyS81E-DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score11(text, pos, child=False):\n",
        "  ''' Identifies third person functor '''\n",
        "  returnal = 0\n",
        "  if re.search('VERB', pos):\n",
        "    pos_list = pos.split(\" \")\n",
        "    words = text.split(\" \")\n",
        "    indices = [i for i, x in enumerate(pos_list) if x == 'VERB']\n",
        "    for inx in indices:\n",
        "      if words[inx].endswith(\"s\") and (score8(words[inx], 'VERB') + score7(words[inx],'VERB')) == 0 and not words[inx].endswith(\"'s\"):\n",
        "        if child:\n",
        "          returnal += 8\n",
        "        else: returnal += 11\n",
        "\n",
        "  return returnal\n",
        "  #3rd person singular\n",
        "  #NP/Pron+sing - V+tns - (Adv)"
      ],
      "metadata": {
        "id": "D-bB4zP5FG-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def total_score(sentence, pos_sentence, child=False):\n",
        "  '''\n",
        "  Calculates and returns total score by calling all scoring functions\n",
        "  To be averaged over sentences or tokens\n",
        "  '''\n",
        "\n",
        "  score = 0\n",
        "\n",
        "  score += score1(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score2(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score3(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score4(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score5(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score6(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score7(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score8(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score9(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score10(sentence.lower(), pos_sentence, child=child)\n",
        "  score += score11(sentence.lower(), pos_sentence, child=child)\n",
        "\n",
        "  return score"
      ],
      "metadata": {
        "id": "WULhhqe2Gdi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def diff_score(sequence, pos, sent_tokenizer, child=False):\n",
        "  '''\n",
        "  Calculates difficulty score for a sequence given sequence, pos and sentence_tokenizer trained on full dataset\n",
        "  Returns sentence averaged difficulty score and token averaged difficulty score\n",
        "  child=True if FLA\n",
        "  '''\n",
        "\n",
        "  #sentence tokenizer\n",
        "  #chunk pos in equal lists as sentences\n",
        "  sentences = sent_tokenizer.tokenize(sequence)\n",
        "  lengths = []\n",
        "  for sent in sentences:\n",
        "    lengths.append(len(sent.split(\" \")))\n",
        "\n",
        "  splitpos = pos.split(\" \")\n",
        "  posses = []\n",
        "  start = 0\n",
        "  for i in lengths:\n",
        "    posses.append(\" \".join(splitpos[start:start + i]))\n",
        "    start = i + start\n",
        "\n",
        "  total = 0\n",
        "  for sent, pos in zip(sentences, posses):\n",
        "    if len(sent.split(\" \")) == len(pos.split(\" \")):\n",
        "      total += total_score(sent, pos, child=child)\n",
        "    else:\n",
        "      print(sent, pos)\n",
        "\n",
        "  diff_sent = total / len(sentences)\n",
        "  diff_tokens = total / len(sequence.split(\" \"))\n",
        "\n",
        "  return diff_sent, diff_tokens"
      ],
      "metadata": {
        "id": "ATG6O7mZF0x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scoring(filepath, child=False):\n",
        "  '''\n",
        "  Takes file with sequences and scores sequences.\n",
        "  Returns dataframe with sequence, pos, sentence averaged difficulty score, token averaged difficulty score\n",
        "  child=True if FLA\n",
        "  '''\n",
        "\n",
        "  with open(filepath) as infile:\n",
        "    full = infile.read()\n",
        "\n",
        "  combined = full.split(\"\\n\")\n",
        "\n",
        "  sent_tokenizer = PunktSentenceTokenizer(full)\n",
        "\n",
        "  sequences = []\n",
        "  posses = []\n",
        "\n",
        "  for sentence in combined:\n",
        "    sent = sentence.split(\"\\t\")\n",
        "    if len(sent) > 1:\n",
        "      sequences.append(sent[0])\n",
        "      posses.append(sent[1])\n",
        "\n",
        "  sent_diff_scores = []\n",
        "  tokens_diff_scores = []\n",
        "  for seq, pos in zip(sequences, posses):\n",
        "    if seq == \"\":\n",
        "      sent_diff_scores.append(0)\n",
        "      tokens_diff_scores.append(0)\n",
        "    else:\n",
        "      diff_sent, diff_tokens = diff_score(seq, pos, sent_tokenizer, child=child)\n",
        "      sent_diff_scores.append(diff_sent)\n",
        "      tokens_diff_scores.append(diff_tokens)\n",
        "\n",
        "# dictionary of lists\n",
        "  dict = {'sequence' : sequences,\n",
        "          'pos' : posses,\n",
        "          'sentence_score': sent_diff_scores,\n",
        "          'tokens_score': tokens_diff_scores}\n",
        "\n",
        "  combined_df = pd.DataFrame(dict)\n",
        "  combined_df.to_csv('scored_sequences_postagged.csv')\n",
        "\n",
        "  return combined_df"
      ],
      "metadata": {
        "id": "cK2TXo-MHSpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup(ranking):\n",
        "  \"\"\"\n",
        "  Turns tokenized sentences back into regular sentences\n",
        "  \"\"\"\n",
        "\n",
        "  cleaned_seqs = []\n",
        "  for seq in ranking['sequence']:\n",
        "    seq = re.sub(r\" ([!\\\"#\\$%&\\'\\(\\)\\*\\+,-\\./:;<=>\\?@\\[\\\\\\]\\^_`{\\|}~])\", r\"\\1\", seq)\n",
        "    seq = re.sub(r\" (n't)\", r\"\\1\", seq)\n",
        "    cleaned_seqs.append(seq)\n",
        "\n",
        "  ranking['sequence'] = cleaned_seqs\n",
        "  return ranking"
      ],
      "metadata": {
        "id": "DRVMBJqtHGts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ranking(filepath, score='sentence_score', ascending=True, child=False):\n",
        "  \"\"\"\n",
        "  Takes scored sequences, ranks them and saves ranked sequences to file.\n",
        "  score='token_score' if token averaged\n",
        "  ascending=False if reversed\n",
        "  child=True if FLA\n",
        "  \"\"\"\n",
        "\n",
        "  scoring = pd.read_csv(filepath)\n",
        "  scoring = scoring.dropna()\n",
        "\n",
        "  ranking = scoring.sort_values(by=[score], ascending=ascending)\n",
        "  ranking = cleanup(ranking)\n",
        "\n",
        "  writefile = 'ranked_sequences_postagged'\n",
        "  if not ascending:\n",
        "    writefile += \"_reversed\"\n",
        "  if score =='tokens_score':\n",
        "    writefile += \"_token\"\n",
        "  if child:\n",
        "    writefile += \"_child\"\n",
        "\n",
        "  writefile += \".csv\"\n",
        "  ranking.to_csv(writefile)\n",
        "  return ranking"
      ],
      "metadata": {
        "id": "4kh_yaTgF0wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_curriculum(filepath):\n",
        "  \"\"\"\n",
        "  Takes ranked sequences as input, randomly shuffles and saves to file\n",
        "  \"\"\"\n",
        "\n",
        "  scoring = pd.read_csv(filepath)\n",
        "  scoring = scoring.dropna()\n",
        "  ranking = scoring.sample(frac = 1)\n",
        "  ranking = cleanup(ranking)\n",
        "\n",
        "  writefile = 'random_curriculum.csv'\n",
        "  ranking.to_csv(writefile)\n",
        "  return ranking"
      ],
      "metadata": {
        "id": "z5DQyA0m2rMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filtered_curriculum(filepath):\n",
        "  '''\n",
        "  Takes ranked sequences and deleted any with a difficulty score of 0, saves to file\n",
        "  '''\n",
        "\n",
        "  sequences = pd.read_csv(filepath)\n",
        "  filtered = sequences.loc[sequences['sentence_score'] != 0]\n",
        "  newpath = \"filtered_\" + filepath\n",
        "  filtered.to_csv(newpath)\n",
        "\n",
        "  return filtered"
      ],
      "metadata": {
        "id": "3nKb8onlynb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pipeline(child = False):\n",
        "  ''' full pipeline for SLA datasets, child=True for FLA datasets '''\n",
        "\n",
        "  scoring('clean_pred3', child=child)\n",
        "  curriculum = ranking('scored_sequences.csv', child=child)\n",
        "  reversed = ranking('ranked_sequences.csv', ascending=False, child=child)\n",
        "  token = ranking('ranked_sequences.csv', score='tokens_score', child=child)\n",
        "  random = random_curriculum('ranked_sequences.csv', child=child)\n",
        "  filtered = filtered_curriculum('ranked_sequences.csv', child=child)"
      ],
      "metadata": {
        "id": "CL6CirD6XZq1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}